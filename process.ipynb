{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Processing\n",
    "\n",
    "This notebook processes scraped time series data by:\n",
    "1. Loading the raw scraped data\n",
    "2. Cleaning and standardizing date formats\n",
    "3. Handling missing values\n",
    "4. Creating time-series visualizations\n",
    "5. Exporting cleaned data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Scraped Data\n",
    "\n",
    "Load the raw data from `scraped_data.csv` that was created by `scrape.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped data\n",
    "INPUT_FILE = 'scraped_data.csv'\n",
    "OUTPUT_FILE = 'cleaned_timeseries.csv'\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Loaded {len(df_raw)} rows from {INPUT_FILE}\")\n",
    "    print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {INPUT_FILE} not found.\")\n",
    "    print(\"Please run 'python scrape.py' first to fetch the data.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows to understand the data structure\n",
    "print(\"Raw Data Preview:\")\n",
    "print(\"=\" * 50)\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(\"-\" * 30)\n",
    "print(df_raw.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"-\" * 30)\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean Date Formats\n",
    "\n",
    "The raw data may contain dates in various formats. We'll standardize them to a consistent datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse dates in multiple formats and return a standardized datetime.\n",
    "    \n",
    "    Supported formats:\n",
    "    - 2024-01-02 (ISO format)\n",
    "    - 01/02/2024 (US format)\n",
    "    - 01-02-2024 (Dash format)\n",
    "    - 2024/01/02 (Slash ISO)\n",
    "    - Jan 02, 2024 (Month name)\n",
    "    \"\"\"\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d',      # 2024-01-02\n",
    "        '%m/%d/%Y',      # 01/02/2024\n",
    "        '%m-%d-%Y',      # 01-02-2024\n",
    "        '%Y/%m/%d',      # 2024/01/02\n",
    "        '%b %d, %Y',     # Jan 02, 2024\n",
    "        '%B %d, %Y',     # January 02, 2024\n",
    "        '%d-%m-%Y',      # 02-01-2024 (European)\n",
    "        '%d/%m/%Y',      # 02/01/2024 (European)\n",
    "    ]\n",
    "    \n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Try pandas parser as fallback\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"Date parsing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Parse dates using our custom function\n",
    "print(\"Parsing dates...\")\n",
    "print(\"\\nSample of original dates:\")\n",
    "print(df['Date'].head(10).tolist())\n",
    "\n",
    "df['Date'] = df['Date'].apply(parse_date)\n",
    "\n",
    "print(\"\\nSample of parsed dates:\")\n",
    "print(df['Date'].head(10).tolist())\n",
    "\n",
    "# Check for any parsing failures\n",
    "failed_parses = df['Date'].isna().sum()\n",
    "if failed_parses > 0:\n",
    "    print(f\"\\nWarning: {failed_parses} dates could not be parsed\")\n",
    "else:\n",
    "    print(\"\\nAll dates parsed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Numeric Columns\n",
    "\n",
    "Ensure all price and volume columns are properly typed as numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# Convert to numeric, coercing errors to NaN\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"{col}: converted to numeric\")\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing dates (critical field)\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=['Date'])\n",
    "dropped_dates = initial_rows - len(df)\n",
    "\n",
    "# Forward fill missing numeric values (common for time series)\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(f\"\\nDropped {dropped_dates} rows with missing dates\")\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sort and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Basic validation\n",
    "print(\"Data Validation:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Total trading days: {len(df)}\")\n",
    "print(f\"\\nPrice Statistics:\")\n",
    "print(df[['Open', 'High', 'Low', 'Close']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity (High >= Low, etc.)\n",
    "integrity_issues = []\n",
    "\n",
    "if 'High' in df.columns and 'Low' in df.columns:\n",
    "    invalid_hl = (df['High'] < df['Low']).sum()\n",
    "    if invalid_hl > 0:\n",
    "        integrity_issues.append(f\"{invalid_hl} rows where High < Low\")\n",
    "\n",
    "if 'High' in df.columns and 'Close' in df.columns:\n",
    "    invalid_hc = (df['High'] < df['Close']).sum()\n",
    "    if invalid_hc > 0:\n",
    "        integrity_issues.append(f\"{invalid_hc} rows where High < Close\")\n",
    "\n",
    "if 'Low' in df.columns and 'Close' in df.columns:\n",
    "    invalid_lc = (df['Low'] > df['Close']).sum()\n",
    "    if invalid_lc > 0:\n",
    "        integrity_issues.append(f\"{invalid_lc} rows where Low > Close\")\n",
    "\n",
    "if integrity_issues:\n",
    "    print(\"Data Integrity Issues Found:\")\n",
    "    for issue in integrity_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"Data integrity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Visualization\n",
    "\n",
    "Create visualizations to understand trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Closing Price Over Time\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df['Date'], df['Close'], color='#2196F3', linewidth=1.5, label='Close Price')\n",
    "ax1.fill_between(df['Date'], df['Close'], alpha=0.3, color='#2196F3')\n",
    "ax1.set_title('Closing Price Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax1.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: OHLC Range (High-Low Spread)\n",
    "ax2 = axes[1]\n",
    "ax2.fill_between(df['Date'], df['Low'], df['High'], alpha=0.4, color='#4CAF50', label='High-Low Range')\n",
    "ax2.plot(df['Date'], df['Open'], color='#FF9800', linewidth=1, linestyle='--', label='Open', alpha=0.8)\n",
    "ax2.plot(df['Date'], df['Close'], color='#2196F3', linewidth=1, label='Close')\n",
    "ax2.set_title('Price Range (Open, High, Low, Close)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Price ($)')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax2.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: Trading Volume\n",
    "ax3 = axes[2]\n",
    "ax3.bar(df['Date'], df['Volume'], color='#9C27B0', alpha=0.7, width=0.8)\n",
    "ax3.set_title('Trading Volume Over Time', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Volume')\n",
    "ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax3.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Format y-axis for volume (millions)\n",
    "ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('timeseries_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'timeseries_visualization.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Daily returns\n",
    "df['Daily_Return'] = df['Close'].pct_change() * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "colors = ['#4CAF50' if x >= 0 else '#F44336' for x in df['Daily_Return'].fillna(0)]\n",
    "ax.bar(df['Date'], df['Daily_Return'], color=colors, alpha=0.7, width=0.8)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_title('Daily Returns (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Return (%)')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print return statistics\n",
    "print(\"\\nDaily Return Statistics:\")\n",
    "print(f\"  Mean: {df['Daily_Return'].mean():.4f}%\")\n",
    "print(f\"  Std Dev: {df['Daily_Return'].std():.4f}%\")\n",
    "print(f\"  Min: {df['Daily_Return'].min():.4f}%\")\n",
    "print(f\"  Max: {df['Daily_Return'].max():.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataframe for export\n",
    "# Remove the Daily_Return column if you only want the core data\n",
    "df_export = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "\n",
    "# Format date as ISO string for CSV export\n",
    "df_export['Date'] = df_export['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Round numeric values for cleaner output\n",
    "df_export['Open'] = df_export['Open'].round(2)\n",
    "df_export['High'] = df_export['High'].round(2)\n",
    "df_export['Low'] = df_export['Low'].round(2)\n",
    "df_export['Close'] = df_export['Close'].round(2)\n",
    "df_export['Volume'] = df_export['Volume'].astype(int)\n",
    "\n",
    "print(\"Final cleaned data preview:\")\n",
    "df_export.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df_export.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Cleaned data exported to '{OUTPUT_FILE}'\")\n",
    "print(f\"\\nExport Summary:\")\n",
    "print(f\"  Total rows: {len(df_export)}\")\n",
    "print(f\"  Columns: {list(df_export.columns)}\")\n",
    "print(f\"  Date range: {df_export['Date'].iloc[0]} to {df_export['Date'].iloc[-1]}\")\n",
    "print(f\"  File size: {os.path.getsize(OUTPUT_FILE) / 1024:.2f} KB\" if 'os' in dir() else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the exported file\n",
    "import os\n",
    "\n",
    "df_verify = pd.read_csv(OUTPUT_FILE)\n",
    "print(\"Verification - Reading exported file:\")\n",
    "print(f\"  Rows: {len(df_verify)}\")\n",
    "print(f\"  Columns: {list(df_verify.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_verify.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df_verify.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed the following tasks:\n",
    "\n",
    "1. **Loaded** raw scraped data from `scraped_data.csv`\n",
    "2. **Cleaned** date formats from multiple input formats to standardized ISO format\n",
    "3. **Validated** numeric columns and handled missing values\n",
    "4. **Created** time-series visualizations showing price trends and volume\n",
    "5. **Exported** cleaned data to `cleaned_timeseries.csv`\n",
    "\n",
    "The cleaned dataset is now ready for further analysis, modeling, or integration with other systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
